{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist,pos_tag\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from operator import itemgetter\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convergence threshold is the maximum error in score convergence of TextRank\n",
    "CONVERGENCE_THRESHOLD = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of all nouns\n",
    "NOUNS = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    '''\n",
    "    The master class for our Document Summerization module.\n",
    "    Incorporates all features related to Document\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, document):\n",
    "        self.document = document\n",
    "        #self.sents = sent_tokenize(self.document)\n",
    "        self.sents = self.document.split('\\n')\n",
    "        self.word_freq = FreqDist(clean(self.document))\n",
    "        self.graph = None\n",
    "        self.params = { 'thresh': 0.0 }\n",
    "                \n",
    "    def __str__(self):\n",
    "        return self.document\n",
    "    \n",
    "    \n",
    "    def statistical_sim(self, sent1, sent2):\n",
    "        '''\n",
    "        Statistical similarity between sentences\n",
    "        based on the cosine method\n",
    "        Returns: float (the cosine similarity b/w sent1 and sent2)\n",
    "        '''\n",
    "        sent_token1 = Counter(sent1)\n",
    "        sent_token2 = Counter(sent2)\n",
    "        \n",
    "        intxn = set(sent_token1) & set(sent_token2)\n",
    "        numerator = sum([sent_token1[x] * sent_token2[x] for x in intxn])\n",
    "        \n",
    "        mod1 = sum([sent_token1[x]**2 for x in sent_token1.keys()])\n",
    "        mod2 = sum([sent_token2[x]**2 for x in sent_token2.keys()])\n",
    "        denominator = sqrt(mod1)*sqrt(mod2)\n",
    "        \n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "\n",
    "        return float(numerator)/denominator\n",
    "    \n",
    "    \n",
    "    def semantic_sim(self, sent1, sent2):\n",
    "        '''\n",
    "        A semantic similarity score between two sentences\n",
    "        based on WordNet\n",
    "        Returns: float (the semantic similarity measure)\n",
    "        '''\n",
    "        score = 0\n",
    "        sent1 = [word for word in sent1 if word in NOUNS]\n",
    "        sent2 = [word for word in sent2 if word in NOUNS]\n",
    "        for t1 in sent1:\n",
    "            for t2 in sent2:\n",
    "                score += semantic_score(t1,t2)\n",
    "        try:\n",
    "            return score/(len(sent1 + sent2))  \n",
    "        except:\n",
    "            return 10000\n",
    "    \n",
    "    \n",
    "    def construct_graph(self):\n",
    "        '''\n",
    "        Constructs the word similarity graph\n",
    "        '''\n",
    "        connected = []\n",
    "        for pair in combinations(self.sents, 2):\n",
    "            cpair = clean(pair[0]), clean(pair[1])\n",
    "            weight = self.statistical_sim(*cpair) + \\\n",
    "                     self.semantic_sim(*cpair)\n",
    "            connected.append((pair[0], pair[1], weight))\n",
    "        self.graph = draw_graph(connected, self.params['thresh'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def clean(sent):\n",
    "    '''\n",
    "    A utility function that returns a a list of words in a sentence\n",
    "    after cleaning it. Gets rid off uppper-case, punctuations, \n",
    "    stop words, etc.\n",
    "    Returns: list (a list of cleaned words in sentence)\n",
    "    '''\n",
    "    words =  sent.lower() \n",
    "    words = re.findall(r'\\w+', words,flags = re.UNICODE | re.LOCALE) \n",
    "    imp_words = filter(lambda x: x not in stopwords.words('english'), words)\n",
    "    return imp_words\n",
    "        \n",
    "def semantic_score(word1, word2):\n",
    "    '''\n",
    "    Semantic score between two words based on WordNet\n",
    "    Returns: float (the semantic score between word1 and word2)\n",
    "    '''\n",
    "    try:\n",
    "        w1 = wn.synset('%s.n.01'%(word1))\n",
    "        w2 = wn.synset('%s.n.01'%(word2))\n",
    "        return wn.path_similarity(w1,w2,simulate_root = False)\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def draw_graph(connected, thresh):\n",
    "    '''\n",
    "    Draws graph as per weights and puts edges if \n",
    "    weight exceed the given thresh\n",
    "    Returns: networkx Graph (nodes are sentences and edges\n",
    "             are statistical and semantic relationships)\n",
    "    '''\n",
    "    nodes = set([n1 for n1, n2, n3 in connected] + \\\n",
    "                [n2 for n1, n2, n3 in connected])\n",
    "    G=nx.Graph()\n",
    "    for node in nodes:\n",
    "        G.add_node(node)\n",
    "    for edge in connected:\n",
    "        if edge[2] > thresh:\n",
    "            G.add_edge(edge[0], edge[1],weight = edge[2])\n",
    "    #plt.figure(figsize=(8,8))\n",
    "    #pos = nx.spring_layout(G)\n",
    "    #nx.draw(G,node_color='#A0CBE2', edge_color='orange',width=1,with_labels=False)\n",
    "    #plt.show()\n",
    "    return G\n",
    "    \n",
    "def textrank_weighted(graph, initial_value=None, damping=0.85):\n",
    "    '''\n",
    "    Calculates PageRank for an undirected graph\n",
    "    Returns: A list of tuples representing sentences and respective\n",
    "    scores in descending order\n",
    "    '''\n",
    "    if initial_value == None:\n",
    "        initial_value = 1.0 / len(graph.nodes())\n",
    "    scores = dict.fromkeys(graph.nodes(), initial_value)\n",
    "\n",
    "    iteration_quantity = 0\n",
    "    for iteration_number in xrange(100):\n",
    "        iteration_quantity += 1\n",
    "        convergence_achieved = 0\n",
    "        for i in graph.nodes():\n",
    "            rank = 1 - damping\n",
    "            for j in graph.neighbors(i):\n",
    "                neighbors_sum = sum([graph.get_edge_data(j, k)['weight'] for k in graph.neighbors(j)])\n",
    "                rank += damping * scores[j] * graph.get_edge_data(j, i)['weight'] / neighbors_sum\n",
    "\n",
    "            if abs(scores[i] - rank) <= CONVERGENCE_THRESHOLD:\n",
    "                convergence_achieved += 1\n",
    "\n",
    "            scores[i] = rank\n",
    "\n",
    "        if convergence_achieved == len(graph.nodes()):\n",
    "            break\n",
    "    return sorted(scores.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter OAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import re\n",
    "import csv as csv\n",
    "from time import strftime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet):\n",
    "    ''' Cleans the tweet by removing hyperlinks\n",
    "        and other unnecessary strings\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(RT @[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|@[A-Za-z0-9]+\",\" \",tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_id(tweets,tag,tweet):\n",
    "    ''' Finds a tweet corresponding to\n",
    "        a given id. The id is obtained\n",
    "        while extracting the tweets '''\n",
    "    for t in tweets[tag]:\n",
    "        if tweet == (t[0]):\n",
    "            return t[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trends(trend):\n",
    "    ''' Returns a list of top 50 trending\n",
    "        hashtags from US region '''\n",
    "    return [(i['name']) for i in trend[0]['trends']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweets(hashtags = [],nb_tweets = 10):\n",
    "    ''' Get nb_tweets from the list hashtags.\n",
    "        The tweets are returned as a dictionary\n",
    "        of list where each list corresponds to\n",
    "        a list of tweets of value hashtag '''\n",
    "    tweets = {}\n",
    "    for tag in hashtags:\n",
    "        Tweets = tweepy.Cursor(api.search, q=tag).items(nb_tweets)\n",
    "        text = [(tweet_cleaner((tweet.text).encode('ascii','ignore')) + '.',tweet.id) for tweet in Tweets]\n",
    "        tweets[tag] = text\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_tweets(tweets = {},top = 3):\n",
    "    ''' Takes in a dictionary of lists of tweets.\n",
    "        Applies extractive summarisation over\n",
    "        each list of tweets and finds the top \n",
    "        tweets. The top tweets are returned as\n",
    "        a dictionary of lists. '''\n",
    "    extract_tweets = {}\n",
    "    for tag in tweets:\n",
    "        temp = [tweet[0] for tweet in tweets[tag]]\n",
    "        string = '\\n'.join(temp)\n",
    "        a = Document(string)\n",
    "        a.construct_graph()\n",
    "        x = textrank_weighted(a.graph)\n",
    "        extract_tweets[tag] = [(i[0],find_id(tweets,tag,i[0])) for i in x[:top]]\n",
    "    return extract_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Filtering the tweets\n",
    "def filter_tweets(tweets):\n",
    "    ''' Filtering out the hashtags with less than 2 tweets and\n",
    "        filtering out all the tweets which have less than 3\n",
    "        words in it or else they would create ruckus in the\n",
    "        extractive summarisation '''\n",
    "    for tag in tweets:\n",
    "        if len(tweets[tag]) < 2:\n",
    "            del tweets[tag]\n",
    "            pass\n",
    "        for tweet in tweets[tag]:\n",
    "            if len(tweet[0].split(' ')) < 4:\n",
    "                tweets[tag].remove(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_csv(extract = {},final = {}):\n",
    "    filename = strftime(\"%m-%d-%Y.csv\")\n",
    "    print \"Saving the data to filename: \"+str(filename)\n",
    "    print \"Appending the data to filename: tokens.csv\"\n",
    "    with open(filename,'wb') as fp:\n",
    "        p = csv.writer(fp)\n",
    "        p.writerow(['hashtag','tweets','main tweet'])\n",
    "        for tag in extract:\n",
    "            p.writerow([tag,new_extract[tag],final[tag]])\n",
    "    with open('tokens.csv','a') as fp:\n",
    "        p = csv.writer(fp)\n",
    "        for tag in extract:\n",
    "            p.writerow([tag,new_extract[tag],final[tag]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_pickel(filename='tokens.csv'):\n",
    "    ''' Converts the csv file generated above to\n",
    "        the pickel format. \n",
    "        Returns None '''\n",
    "    news = []\n",
    "    headlines = []\n",
    "    with open(filename,\"rb\") as fp:\n",
    "            reader = csv.DictReader(fp)\n",
    "            for line in reader:\n",
    "                text = ''\n",
    "                l = line['tweets'][1:-1].split(',')\n",
    "                for i in range(len(l)):\n",
    "                    l[i] = l[i].strip()\n",
    "                    l[i] = l[i].strip('()\\'')\n",
    "                for i in range(0,len(l),2):\n",
    "                    text+=l[i]\n",
    "                news.append(text)\n",
    "                l = line['main tweet'][1:-1].split(',')\n",
    "                for i in range(len(l)):\n",
    "                    l[i] = l[i].strip()\n",
    "                    l[i] = l[i].strip('()\\'')\n",
    "                headlines.append(l[0])\n",
    "    keywords = [None for i in range(len(news))]\n",
    "    print \"Saving to the pickel file named: \"+str('tokens.pkl')\n",
    "    pickle.dump((headlines,news,keywords),open('tokens.pkl',\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display(tweets = {}):\n",
    "    ''' Utility function to display the \n",
    "        tweets present in a dictionary\n",
    "        of list tweets '''\n",
    "    for tag in tweets:\n",
    "        print tag\n",
    "        print '----------------------------------------------------------'\n",
    "        for tweet in tweets[tag]:\n",
    "            print tweet\n",
    "        print '==========================================================='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Najeeb Khan Credentials\n",
    "consumer_key = '3cFJ5hLMswDJOwXfaJGS4eNyS'\n",
    "consumer_secret = 'GRn1itnKbrCdAvHhbpjFgzWXMHePNuhDYz5scjnyhD8fC3Bnqg'\n",
    "access_token = '277893116-tcAWnc62SVdSBLUIqF5R5h92qm2Y0epfQUQJNa4l'\n",
    "access_secret = 'S47f26iaI7L0Z5AyT2NQqtsmitrMn70nZG2H5n5dyqM4C'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)\n",
    "trend = api.trends_place(23424977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtags = get_trends(trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = get_tweets(hashtags,nb_tweets=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_copy = tweets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_tweets(tweets_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract = get_top_tweets(tweets_copy,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_tweets(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = get_top_tweets(extract,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' Removing the main tweet from the summarised tweet\n",
    "    so as to facilitate the learning of the neural\n",
    "    network. Otherwise the network may learn to pick\n",
    "    up the main tweet as it is '''\n",
    "new_extract = extract.copy()\n",
    "for tag in extract:\n",
    "    new_extract[tag].remove(final[tag][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_csv(new_extract,final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "convert_pickel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y,keywords = pickle.load(open('tokens.pkl',\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    print \"H: \"+str(X[i])\n",
    "    print \"T: \"+str(y[i])\n",
    "    print \"-----------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string = y[0]\n",
    "print string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
